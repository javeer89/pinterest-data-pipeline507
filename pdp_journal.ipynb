{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pinterest Data Pipeline**\n",
    "*Pinterest Data Pipeline project using Databricks, Spark, Airflow, Kinesis, Kafka, API Gateway*\n",
    "\n",
    "Pinterest crunches billions of data points every day to decide how to provide more value to their users.\\\n",
    "In this project, we'll create a similar system using the AWS Cloud.\n",
    "\n",
    "To start with we used a **[emulation script](script/user_post_emulation.py)** to recreate sample data of Pinterest infrastructure. The data is broken into 3 parts; **Post**, **Geolcation** and **User**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 1 // Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinterest Post Data\n",
    "This Data contains details of what a post would contain, such as image title, description, auther (poster_name) as well as additional details such as follower_count and tags. See sample data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "{   'index': 9198,\n",
    "    'unique_id': '4beee221-e7c2-4e5d-93d9-a0e98e3450a0',\n",
    "    'title': 'Find the best global talent.',\n",
    "    'description': 'No description available',\n",
    "    'poster_name': 'Fiverr',\n",
    "    'follower_count': '565k',\n",
    "    'tag_list': 'Hand Tattoos,Dope Tattoos,Pretty Tattoos,Beautiful Tattoos,Body Art Tattoos,Small Tattoos,Tattoos For Guys,Tattoos For Women,Flower Tattoos',\n",
    "    'is_image_or_video': 'image',\n",
    "    'image_src': 'https://i.pinimg.com/originals/f9/08/67/f908679c6fd45aed6ab23b482728fa83.jpg',\n",
    "    'downloaded': 1,\n",
    "    'save_location': 'Local save in /data/tattoos',\n",
    "    'category': 'tattoos'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geolaction Data\n",
    "Contains details of where the post was made from and where the author is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "{   'ind': 9198,\n",
    "    'timestamp': datetime.datetime(2019, 4, 7, 22, 11, 2),\n",
    "    'latitude': -12.1295,\n",
    "    'longitude': -29.9199,\n",
    "    'country': 'Afghanistan'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Data\n",
    "Contains details of the author's actual name and details as opposed to the alias presented under \"poster_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "{   'ind': 9198,\n",
    "    'first_name': 'Amber',\n",
    "    'last_name': 'Chen',\n",
    "    'age': 22,\n",
    "    'date_joined': datetime.datetime(2015, 12, 30, 5, 21, 14)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 2 // Set up EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to commence with the batch processing, we will first need to set up a Kafka cluster. We will do this on our EC2 instance on the AWS Cloud service. You will require setting up a IAM account alongside AWS Access Code and Secret Access Code. Once your accounts are configured and set-up, you will establish a connection to your EC2 VPC machine through the terminal. To do this, you will require creating a .pem key file containing the key-code for your connection.\n",
    "\n",
    "Go to\n",
    "```\n",
    "EC2 > Instances > <instance-id> > Connect to instance\n",
    "```\n",
    "on your Amazon Cloud Service and you will find instructions on how to set up your connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 3 // Set up Kafka Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run the Kafka Cluster to ingest the data. We need to adjust the client.properties to connect with out IAM authentication. This is so that the ingested data will get stored inside of our S3 storage.\n",
    "\n",
    "You can create the client.properties file by using the ```nano``` command and insert the below code with your IAM and Access Role details.\n",
    "```\n",
    "ec2-user > kafka_[version-no.] > bin \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "# Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"Your Access Role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 4 // Create Kafka Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create our Kafka Topics which will tell our data where to go upon ingestion.\n",
    "\n",
    "As there are 3 pieces of data we will repeat the below code 3 times. The ```/kafka-topics.sh``` file can be located inside the same ```ec2/kafka.../bin```. \\\n",
    "You can locate your ```BootstrapServerString``` inside of your AWS Cloud Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server BootstrapServerString --command-config client.properties --create --topic <topic_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 5 // Connect the MSK Cluster to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the credentials are inside our client.properties we need a means of executing the ingestion process via Kafka. To do this we will set up the connection between our cluster and the S3 database. We will install the CONFLUENT Connect package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "```\n",
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i\n",
    "\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3\n",
    "\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip\n",
    "\n",
    "# copy connector to our S3 bucket\n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://<BUCKET_NAME>/kafka-connect-s3/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a **CUSTOM PLUGIN** inside of the MSK Cluster.\\\n",
    "```Amazon MSK > Customised plugins > 0e2a0bfcc015-plugin```\\\n",
    "Insert the below code into the connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "connector.class=io.confluent.connect.s3.S3SinkConnector\n",
    "\n",
    "# same region as our bucket and cluster\n",
    "s3.region=us-east-1\n",
    "flush.size=1\n",
    "schema.compatibility=NONE\n",
    "tasks.max=3\n",
    "\n",
    "# include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....\n",
    "topics.regex=<YOUR_UUID>.*\n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat\n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "value.converter.schemas.enable=false\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "s3.bucket.name=<BUCKET_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 6 // Configure API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create our Kafka REST Proxy API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Go to the API Gateway Console on the MSK. API Gateway > APIs \\\n",
    "2 - Create a {proxy+} resource \\\n",
    "3 - Next setup the INTEGRATION through the EDIT INTEGRATION and select HTTP proxy \\\n",
    "4 - For the ENDPOINT URL, use the Public IPv4 DNS found in the MSK console for EC2 instances. (Format should be, http://```**KafkaClientEC2InstancePublicDNS**```:8082/{proxy}) \\\n",
    "5 - Finally you are set to DEPLOY the API Gateway\n",
    "6 - Provide a stage Name and details.\n",
    "\n",
    "We will use the **invoke URL** inside of our **[emulation script](script/user_post_emulation.py)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above steps are completed, we will update our Kafka-Rest folder on our EC2 machine and place the following into the Kafka.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "#Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "#Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"Your Access Role\";\n",
    "\n",
    "#Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "#The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KAFKA CONSUMER\n",
    "\n",
    "THIS IS THE ONE THAT WORKS > ESTABLISHES LIVE CONSUMER STREAM\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --topic 0e2a0bfcc015.pin --from-beginning --group students --max-messages 10\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --topic 0e2a0bfcc015.geo --from-beginning --group students --max-messages 10\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --topic 0e2a0bfcc015.user --from-beginning --group students --max-messages 10\n",
    "\n",
    "RUN FROM INSIDE KAFKA BIN\n",
    "cd kafka_2.12-2.8.1/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START KAFKA REST BEFORE SENDING VIA API\n",
    "\n",
    "./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties\n",
    "\n",
    "FROM THE confluent-7.2.0/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEYPAIR TO START\n",
    "\n",
    "ssh -i \"utility/0e2a0bfcc015-key-pair.pem\" ec2-user@ec2-184-73-75-230.compute-1.amazonaws.com\n",
    "\n",
    "cd confluent-7.2.0/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 7 // Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 8 // Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 9 // Create DAG on MWAA Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // 10 // Stream Processing: AWS Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "assert os.path.isdir(airflow_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
