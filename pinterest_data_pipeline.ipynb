{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinterest Data Pipeline\n",
    "This is for the README File in the future. May contain additional notes for myself wherever I have problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Pair\n",
    "Creating the keypar was our first course of action using the provided Account ID, IAM user name and Password to connect to the AWS Account and navigate through the console to locate the appropriate **KeyPairId**.\n",
    "</br>\n",
    "See file [0e2a0bfcc015-key-pair.pem](0e2a0bfcc015-key-pair.pem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to EC2\n",
    "ssh -i \"0e2a0bfcc015-key-pair.pem\" ec2-user@ec2-184-73-75-230.compute-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ssh -i \"0e2a0bfcc015-key-pair.pem\" ec2-user@ec2-184-73-75-230.compute-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then ran code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo yum update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the standard command line commands to navigate around the file directory. <br>\n",
    "Also, you can use \"nano\" to create and open files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET UP CLIENT PROPERTIES INSIDE KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sets up TLS for encryption and SASL for authN.\n",
    "security.protocol = SASL_SSL\n",
    "\n",
    "#Identifies the ASL# S mechanism to use.\n",
    "sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0e2a0bfcc015-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Kafka\n",
    "first ran: sudo yum update\n",
    "<br> then installed Java using: sudo yum install java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64\n",
    "\n",
    "<br> Kafka Target\n",
    "<br> https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
    "</br>\n",
    "May consider following, https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-20-04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created CLIENT PROPERTIES file inside of the EC2 > Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plaintext Apache Zookeeper connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kafka Topic\n",
    "To do this, while located inside the Kafka_Folder/bin use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server BootstrapServerString --command-config client.properties --create --topic <topic_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing the BootstrapServerString for the previously listed BOOTSTRAP SERVER PRIVATE ENDPOINT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINTEREST POST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.pin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST GEO LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST USER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3: Configure EC2 Kafka Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4: Connect MSK Cluster to S3\n",
    "Using Confluent.io Amazon S3 Connector we will link it with our S3 bucket. This will allow data that flows through our Kafka Stream to be stored in our S3 Bucket for larger periods of time to allow for detailed analysis.\n",
    "\n",
    "```\n",
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip\n",
    "# copy connector to our S3 bucket\n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://<BUCKET_NAME>/kafka-connect-s3/\n",
    "```\n",
    "\n",
    "After that's done.\\\n",
    "You can simply create a **\"CUSTOM PLUGIN\"** via the Amazon MSK interface.\\\n",
    "Locate the CONFLUENT connector zip\\\n",
    "Give the plugin a name and details\\\n",
    "CREATE PLUGIN \\\n",
    "\n",
    "Next, create a connector\\\n",
    "Select the PLUGIN you created\\\n",
    "Give it a name & detail\\\n",
    "Choose your MSK Cluster\\\n",
    "Then drop the following into the CONNECTOR CONFIGURATION SETTING\n",
    "\n",
    "```\n",
    "connector.class=io.confluent.connect.s3.S3SinkConnector\n",
    "# same region as our bucket and cluster\n",
    "s3.region=us-east-1\n",
    "flush.size=1\n",
    "schema.compatibility=NONE\n",
    "tasks.max=3\n",
    "# include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....\n",
    "topics.regex=<YOUR_UUID>.*\n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat\n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "value.converter.schemas.enable=false\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "s3.bucket.name=<BUCKET_NAME>\n",
    "```\n",
    "\n",
    "Finally\\\n",
    "Change CONNECTOR TYPE into PROVISIONED\\\n",
    "Select CUSTOM CONFIGURATION for the WORKER CONFIGURATION - then select \"confluent-worker\"\\\n",
    "Select IAM role for ACCESS PERMISSION\\\n",
    "\n",
    "and DONE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
