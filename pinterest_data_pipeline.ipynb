{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinterest Data Pipeline\n",
    "This is for the README File in the future. May contain additional notes for myself wherever I have problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Pair\n",
    "Creating the keypar was our first course of action using the provided Account ID, IAM user name and Password to connect to the AWS Account and navigate through the console to locate the appropriate **KeyPairId**.\n",
    "</br>\n",
    "See file [0e2a0bfcc015-key-pair.pem](0e2a0bfcc015-key-pair.pem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to EC2\n",
    "ssh -i \"0e2a0bfcc015-key-pair.pem\" ec2-user@ec2-184-73-75-230.compute-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ssh -i \"0e2a0bfcc015-key-pair.pem\" ec2-user@ec2-184-73-75-230.compute-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then ran code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo yum update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the standard command line commands to navigate around the file directory. <br>\n",
    "Also, you can use \"nano\" to create and open files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET UP CLIENT PROPERTIES INSIDE KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sets up TLS for encryption and SASL for authN.\n",
    "security.protocol = SASL_SSL\n",
    "\n",
    "#Identifies the ASL# S mechanism to use.\n",
    "sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "# Binds SASL client implementation.\n",
    "sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::584739742957:role/0e2a0bfcc015-ec2-access-role\";\n",
    "\n",
    "# Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "# The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Kafka\n",
    "first ran: sudo yum update\n",
    "<br> then installed Java using: sudo yum install java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64\n",
    "\n",
    "<br> Kafka Target\n",
    "<br> https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
    "</br>\n",
    "May consider following, https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-20-04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created CLIENT PROPERTIES file inside of the EC2 > Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plaintext Apache Zookeeper connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kafka Topic\n",
    "To do this, while located inside the Kafka_Folder/bin use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server BootstrapServerString --command-config client.properties --create --topic <topic_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing the BootstrapServerString for the previously listed BOOTSTRAP SERVER PRIVATE ENDPOINT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINTEREST POST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.pin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST GEO LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST USER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 0e2a0bfcc015.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka TOPICS Created.\n",
    "\n",
    "0e2a0bfcc015.pin\n",
    "0e2a0bfcc015.geo\n",
    "0e2a0bfcc015.user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3: Configure EC2 Kafka Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4: Connect MSK Cluster to S3\n",
    "Using Confluent.io Amazon S3 Connector we will link it with our S3 bucket. This will allow data that flows through our Kafka Stream to be stored in our S3 Bucket for larger periods of time to allow for detailed analysis.\n",
    "\n",
    "```\n",
    "# assume admin user privileges\n",
    "sudo -u ec2-user -i\n",
    "# create directory where we will save our connector \n",
    "mkdir kafka-connect-s3 && cd kafka-connect-s3\n",
    "# download connector from Confluent\n",
    "wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip\n",
    "# copy connector to our S3 bucket\n",
    "aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://<BUCKET_NAME>/kafka-connect-s3/\n",
    "```\n",
    "\n",
    "After that's done.\\\n",
    "You can simply create a **\"CUSTOM PLUGIN\"** via the Amazon MSK interface.\\\n",
    "Locate the CONFLUENT connector zip\\\n",
    "Give the plugin a name and details\\\n",
    "CREATE PLUGIN \\\n",
    "\n",
    "Next, create a connector\\\n",
    "Select the PLUGIN you created\\\n",
    "Give it a name & detail\\\n",
    "Choose your MSK Cluster\\\n",
    "Then drop the following into the CONNECTOR CONFIGURATION SETTING\n",
    "\n",
    "```\n",
    "connector.class=io.confluent.connect.s3.S3SinkConnector\n",
    "# same region as our bucket and cluster\n",
    "s3.region=us-east-1\n",
    "flush.size=1\n",
    "schema.compatibility=NONE\n",
    "tasks.max=3\n",
    "# include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....\n",
    "topics.regex=<YOUR_UUID>.*\n",
    "format.class=io.confluent.connect.s3.format.json.JsonFormat\n",
    "partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "value.converter.schemas.enable=false\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "storage.class=io.confluent.connect.s3.storage.S3Storage\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "s3.bucket.name=<BUCKET_NAME>\n",
    "```\n",
    "\n",
    "Finally\\\n",
    "Change CONNECTOR TYPE into PROVISIONED\\\n",
    "Select CUSTOM CONFIGURATION for the WORKER CONFIGURATION - then select \"confluent-worker\"\\\n",
    "Select IAM role for ACCESS PERMISSION\\\n",
    "\n",
    "and DONE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5: Configure API Gateway\n",
    "To Replicate the Pinterest's experimental data pipeline we will need to build our own API. This API will send data to the MSK cluster, which in turn will be stored in an S3 bucket, using the connector we have built in the previous milestone.\n",
    "\n",
    "##### T1: Build the Kafka REST proxy API\n",
    "\n",
    "1 - Go to the API Gateway Console on the MSK. API Gateway > APIs \\\n",
    "2 - (since for this project API was already made, we will find it using our user-id) \\\n",
    "3 - Create a {proxy+} resource \\\n",
    "4 - Next setup the INTEGRATION through the EDIT INTEGRATION and select HTTP proxy \\\n",
    "5 - For the ENDPOINT URL, use the Public IPv4 DNS found in the MSK console for EC2 instances. (Format should be, http://```**KafkaClientEC2InstancePublicDNS**```:8082/{proxy}) \\\n",
    "6 - Finally you are set to DEPLOY the API Gateway\n",
    "7 - Provide a stage Name and details.\n",
    "\n",
    "invoke URL Created: https://moyj7yazp4.execute-api.us-east-1.amazonaws.com/pinDP\n",
    "\n",
    "\n",
    "\n",
    "##### T2: Setup Kafka REST proxy on the EC2\n",
    "\n",
    "1 - First install confluent package. \\\n",
    "```sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz ``` \\\n",
    "```tar -xvzf confluent-7.2.0.tar.gz```\n",
    "\n",
    "2 - Next open up the Kafka REST properties, located inside the confluent install. \\ \n",
    "```confluent-7.2.0/etc/kafka-rest```\n",
    "\n",
    "3 - Insert the following into the Kafka REST Properties \\\n",
    "```\n",
    "#Sets up TLS for encryption and SASL for authN.\n",
    "client.security.protocol = SASL_SSL\n",
    "\n",
    "#Identifies the SASL mechanism to use.\n",
    "client.sasl.mechanism = AWS_MSK_IAM\n",
    "\n",
    "#Binds SASL client implementation.\n",
    "client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"Your Access Role\";\n",
    "\n",
    "#Encapsulates constructing a SigV4 signature based on extracted credentials.\n",
    "#The SASL client bound by \"sasl.jaas.config\" invokes this class.\n",
    "client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_posting_emulation import *\n",
    "run_infinite_post_data_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE OUTPUT\n",
    "\n",
    "```\n",
    "{'index': 7528, 'unique_id': 'fbe53c66-3442-4773-b19e-d3ec6f54dddf', 'title': 'No Title Data Available', 'description': 'No description available Story format', 'poster_name': 'User Info Error', 'follower_count': 'User Info Error', 'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e', 'is_image_or_video': 'multi-video(story page format)', 'image_src': 'Image src error.', 'downloaded': 0, 'save_location': 'Local save in /data/mens-fashion', 'category': 'mens-fashion'}\n",
    "\n",
    "{'ind': 7528, 'timestamp': datetime.datetime(2020, 8, 28, 3, 52, 47), 'latitude': -89.9787, 'longitude': -173.293, 'country': 'Albania'}\n",
    "\n",
    "{'ind': 7528, 'first_name': 'Abigail', 'last_name': 'Ali', 'age': 20, 'date_joined': datetime.datetime(2015, 10, 24, 11, 23, 51)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KAFKA CONSUMER\n",
    "\n",
    "```\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --topic 0e2a0bfcc015.geo --from-beginning --group students\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --topic 0e2a0bfcc015.geo --from-beginning --max-messages 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START KAFKA REST BEFORE SENDING VIA API\n",
    "\n",
    "./kafka-rest-start /home/ec2-user/Downlads/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties\n",
    "\n",
    "FROM THE confluent-7.2.0/bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
